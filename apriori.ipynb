{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-13T14:38:36.432897Z",
     "end_time": "2023-06-13T14:38:36.442405Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# File path to load"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "file_path = 'transactions_with_parents.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T14:38:37.299426Z",
     "end_time": "2023-06-13T14:38:37.327950Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "IS_PRINT_ = False\n",
    "MIN_SUP = 140\n",
    "MIN_CONF = 0.3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T14:55:54.271050Z",
     "end_time": "2023-06-13T14:55:54.282560Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Apriori algorithm\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class AprioriAlgorithm:\n",
    "    def __init__(self, min_support, min_confidence):\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        self.df_transaction_dataset = None\n",
    "        self.item_counts = None\n",
    "        self.frequent_itemsets = {}\n",
    "        self.transactions_taxonomy_elements_frequency = None\n",
    "\n",
    "    def load_data(self, file_path,size=2000):\n",
    "        self.df_transaction_dataset = pd.read_csv(file_path)\n",
    "        self.df_transaction_dataset = self.df_transaction_dataset.head(size)\n",
    "\n",
    "    ########################################\n",
    "    # Frequency Counting\n",
    "    ########################################\n",
    "    def frequency_counting(self):\n",
    "        item_counts = self.df_transaction_dataset[[col for col in self.df_transaction_dataset.columns if col.isdigit() and len(col) == 4]].sum()\n",
    "        unique_combinations = set()\n",
    "        combination_counts = {}\n",
    "\n",
    "        for index, row in self.df_transaction_dataset.iterrows():\n",
    "            for col in self.df_transaction_dataset.columns:\n",
    "                if col.isdigit() and len(col) == 3:\n",
    "                    value = row[col]\n",
    "                    if value != 0:\n",
    "                        combination_key = (col, value)\n",
    "                        if combination_key in unique_combinations:\n",
    "                            combination_counts[combination_key] += 1\n",
    "                        else:\n",
    "                            unique_combinations.add(combination_key)\n",
    "                            combination_counts[combination_key] = 1\n",
    "\n",
    "        combination_counts_series = pd.Series(combination_counts)\n",
    "        transactions_taxonomy_elements_frequency = pd.concat([item_counts, combination_counts_series])\n",
    "\n",
    "        return transactions_taxonomy_elements_frequency\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Generate Frequent k-itemsets\n",
    "    ########################################\n",
    "    def generate_candidates(self, prev_itemsets, k):\n",
    "        candidates = set()\n",
    "        for itemset1 in prev_itemsets:\n",
    "            for itemset2 in prev_itemsets:\n",
    "                if len(itemset1.union(itemset2)) == k:\n",
    "                    candidates.add(itemset1.union(itemset2))\n",
    "        return candidates\n",
    "\n",
    "    def prune_itemsets(self, itemsets, rejected_itemsets):\n",
    "        pruned_itemsets = {}\n",
    "        if rejected_itemsets:\n",
    "            itemsets = set(itemset for itemset in itemsets if itemset not in rejected_itemsets)\n",
    "        for itemset in itemsets:\n",
    "            support = sum(\n",
    "                all(\n",
    "                    (item[0] in row and row[item[0]] == item[1])\n",
    "                    if isinstance(item, tuple)\n",
    "                    else (item in row and row[item] != 0)\n",
    "                    for item in itemset\n",
    "                )\n",
    "                for _, row in self.df_transaction_dataset.iterrows()\n",
    "            )\n",
    "            if support >= self.min_support:\n",
    "                pruned_itemsets[itemset] = support\n",
    "            else:\n",
    "                rejected_itemsets.add(itemset)\n",
    "        return pruned_itemsets\n",
    "\n",
    "    def generate_frequent_itemsets(self):\n",
    "        frequent_1_itemsets = {}\n",
    "        rejected_itemsets = set()\n",
    "\n",
    "        for item, count in self.transactions_taxonomy_elements_frequency.items():\n",
    "            if count >= self.min_support:\n",
    "                frequent_1_itemsets[frozenset([item])] = count\n",
    "\n",
    "        self.frequent_itemsets[1] = frequent_1_itemsets\n",
    "\n",
    "        k = 2\n",
    "        while self.frequent_itemsets[k - 1]:\n",
    "            candidate_itemsets = self.generate_candidates(self.frequent_itemsets[k - 1].keys(), k)\n",
    "            pruned_itemsets = self.prune_itemsets(candidate_itemsets, rejected_itemsets)\n",
    "            self.frequent_itemsets[k] = pruned_itemsets\n",
    "            k += 1\n",
    "\n",
    "    ########################################\n",
    "    # Generate Association Rules\n",
    "    ########################################\n",
    "    def generate_association_rules(self):\n",
    "        association_rules = []\n",
    "        frequent_itemsets_to_create_rules = self.frequent_itemsets[len(self.frequent_itemsets) - 2]\n",
    "\n",
    "        for itemset in frequent_itemsets_to_create_rules.items():\n",
    "            support_without_tuple = itemset[1]\n",
    "            itemset = itemset[0]\n",
    "            new_itemset = True\n",
    "            last_support_for_tuple = None\n",
    "            subsets = self.get_subsets(itemset)\n",
    "\n",
    "            for subset in subsets:\n",
    "                antecedent = subset\n",
    "                consequent = itemset - subset\n",
    "                if len(antecedent) >= 2 or isinstance(antecedent, tuple) or any(\n",
    "                        isinstance(x, tuple) for x in consequent):\n",
    "                    antecedent_support = sum(\n",
    "                        all(\n",
    "                            (row[item[0]] > item[1])\n",
    "                            if isinstance(item, tuple)\n",
    "                            else (row[item] == 1)\n",
    "                            for item in antecedent\n",
    "                        )\n",
    "                        for _, row in self.df_transaction_dataset.iterrows()\n",
    "                    )\n",
    "\n",
    "                    if new_itemset == True:\n",
    "                        new_itemset = False\n",
    "                        base_support = sum(\n",
    "                            all(\n",
    "                                (row[item[0]] > item[1])\n",
    "                                if isinstance(item, tuple)\n",
    "                                else (row[item] == 1)\n",
    "                                for item in itemset\n",
    "                            )\n",
    "                            for _, row in self.df_transaction_dataset.iterrows()\n",
    "                        )\n",
    "\n",
    "                        last_support_for_tuple = base_support\n",
    "                    support = last_support_for_tuple\n",
    "                    if antecedent_support > 0:\n",
    "                        confidence = support / antecedent_support\n",
    "                        if confidence >= self.min_confidence:\n",
    "                            association_rules.append((antecedent, consequent, support, confidence))\n",
    "                else:\n",
    "                    antecedent_support = self.frequent_itemsets[len(antecedent)][antecedent]\n",
    "                    confidence = support_without_tuple / antecedent_support\n",
    "                    if confidence >= self.min_confidence:\n",
    "                        association_rules.append((antecedent, consequent, support_without_tuple, confidence))\n",
    "\n",
    "        return association_rules\n",
    "\n",
    "    def get_subsets(self, itemset):\n",
    "        subsets = []\n",
    "        itemset = list(itemset)\n",
    "        num_items = len(itemset)\n",
    "\n",
    "        for i in range(1, num_items):\n",
    "            subsets.extend(itertools.combinations(itemset, i))\n",
    "\n",
    "        return [frozenset(subset) for subset in subsets]\n",
    "\n",
    "    def run_algorithm(self, file_path,dataset_size=2000):\n",
    "        self.load_data(file_path,dataset_size)\n",
    "        self.transactions_taxonomy_elements_frequency = self.frequency_counting()\n",
    "        self.generate_frequent_itemsets()\n",
    "        association_rules = self.generate_association_rules()\n",
    "        return self.transactions_taxonomy_elements_frequency, association_rules"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T14:55:55.277615Z",
     "end_time": "2023-06-13T14:55:55.286575Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class AprioriAlgorithmRunner:\n",
    "    def __init__(self, min_support, min_confidence):\n",
    "        self.apriori_algorithm = AprioriAlgorithm(min_support, min_confidence)\n",
    "\n",
    "    def run_algorithm(self, file_path, dataset_size=2000):\n",
    "        return self.apriori_algorithm.run_algorithm(file_path, dataset_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T14:55:55.569330Z",
     "end_time": "2023-06-13T14:55:55.593828Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "runner = AprioriAlgorithmRunner(min_support=MIN_SUP, min_confidence=MIN_CONF)\n",
    "transactions_taxonomy_elements_frequency, association_rules = runner.run_algorithm(file_path=file_path,dataset_size = 5000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T14:55:56.439273Z",
     "end_time": "2023-06-13T15:02:06.510971Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Counting Results:\n",
      "1001          19\n",
      "1002         175\n",
      "1003           2\n",
      "1004           8\n",
      "1005          47\n",
      "            ... \n",
      "(100, 14)      1\n",
      "(132, 3)       1\n",
      "(213, 1)       4\n",
      "(130, 6)       1\n",
      "(303, 3)       1\n",
      "Length: 1394, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Frequency Counting Results:\")\n",
    "print(transactions_taxonomy_elements_frequency)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T15:02:06.510971Z",
     "end_time": "2023-06-13T15:02:06.525978Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Association Rules:\n",
      "Antecedent: ('132', 1), Consequent: ('130', 1), ('100', 2), ('150', 1), Support: 28, Confidence: 0.5384615384615384\n",
      "Antecedent: ('132', 1), ('150', 1), Consequent: ('130', 1), ('100', 2), Support: 28, Confidence: 1.0\n",
      "Antecedent: ('130', 1), ('132', 1), Consequent: ('100', 2), ('150', 1), Support: 28, Confidence: 0.5384615384615384\n",
      "Antecedent: ('100', 2), ('132', 1), Consequent: ('130', 1), ('150', 1), Support: 28, Confidence: 0.6363636363636364\n",
      "Antecedent: ('130', 1), ('132', 1), ('150', 1), Consequent: ('100', 2), Support: 28, Confidence: 1.0\n",
      "Antecedent: ('100', 2), ('132', 1), ('150', 1), Consequent: ('130', 1), Support: 28, Confidence: 1.0\n",
      "Antecedent: ('130', 1), ('100', 2), ('132', 1), Consequent: ('150', 1), Support: 28, Confidence: 0.6363636363636364\n",
      "Antecedent: ('154', 1), Consequent: 1113, ('100', 1), ('150', 1), Support: 17, Confidence: 0.7727272727272727\n",
      "Antecedent: 1113, ('154', 1), Consequent: ('100', 1), ('150', 1), Support: 17, Confidence: 1.0\n",
      "Antecedent: ('100', 1), ('154', 1), Consequent: 1113, ('150', 1), Support: 17, Confidence: 0.7727272727272727\n",
      "Antecedent: ('154', 1), ('150', 1), Consequent: 1113, ('100', 1), Support: 17, Confidence: 0.7727272727272727\n",
      "Antecedent: 1113, ('100', 1), ('154', 1), Consequent: ('150', 1), Support: 17, Confidence: 1.0\n",
      "Antecedent: 1113, ('154', 1), ('150', 1), Consequent: ('100', 1), Support: 17, Confidence: 1.0\n",
      "Antecedent: ('100', 1), ('154', 1), ('150', 1), Consequent: 1113, Support: 17, Confidence: 0.7727272727272727\n",
      "Antecedent: ('230', 1), Consequent: ('100', 1), ('200', 1), ('150', 1), Support: 200, Confidence: 0.34305317324185247\n",
      "Antecedent: ('230', 1), ('100', 1), Consequent: ('200', 1), ('150', 1), Support: 200, Confidence: 0.8163265306122449\n",
      "Antecedent: ('230', 1), ('200', 1), Consequent: ('100', 1), ('150', 1), Support: 200, Confidence: 0.34305317324185247\n",
      "Antecedent: ('230', 1), ('150', 1), Consequent: ('100', 1), ('200', 1), Support: 200, Confidence: 1.0\n",
      "Antecedent: ('100', 1), ('200', 1), Consequent: ('230', 1), ('150', 1), Support: 200, Confidence: 0.3795066413662239\n",
      "Antecedent: ('200', 1), ('150', 1), Consequent: ('230', 1), ('100', 1), Support: 200, Confidence: 0.48661800486618007\n",
      "Antecedent: ('230', 1), ('100', 1), ('200', 1), Consequent: ('150', 1), Support: 200, Confidence: 0.8163265306122449\n",
      "Antecedent: ('230', 1), ('100', 1), ('150', 1), Consequent: ('200', 1), Support: 200, Confidence: 1.0\n",
      "Antecedent: ('230', 1), ('200', 1), ('150', 1), Consequent: ('100', 1), Support: 200, Confidence: 1.0\n",
      "Antecedent: ('100', 1), ('200', 1), ('150', 1), Consequent: ('230', 1), Support: 200, Confidence: 0.48661800486618007\n",
      "Antecedent: ('210', 1), Consequent: ('200', 2), ('230', 1), 2010, Support: 27, Confidence: 0.3\n",
      "Antecedent: ('210', 1), ('230', 1), Consequent: ('200', 2), 2010, Support: 27, Confidence: 0.9\n",
      "Antecedent: ('200', 2), ('210', 1), Consequent: ('230', 1), 2010, Support: 27, Confidence: 0.4426229508196721\n",
      "Antecedent: ('210', 1), 2010, Consequent: ('200', 2), ('230', 1), Support: 27, Confidence: 0.36486486486486486\n",
      "Antecedent: ('200', 2), ('210', 1), ('230', 1), Consequent: 2010, Support: 27, Confidence: 0.9\n",
      "Antecedent: ('210', 1), ('230', 1), 2010, Consequent: ('200', 2), Support: 27, Confidence: 1.0\n",
      "Antecedent: ('200', 2), ('210', 1), 2010, Consequent: ('230', 1), Support: 27, Confidence: 0.5094339622641509\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAssociation Rules:\")\n",
    "for antecedent, consequent, support, confidence in association_rules:\n",
    "    antecedent_str = ', '.join([str(item) for item in antecedent])\n",
    "    consequent_str = ', '.join([str(item) for item in consequent])\n",
    "    print(f\"Antecedent: {antecedent_str}, Consequent: {consequent_str}, Support: {support}, Confidence: {confidence}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T15:02:14.085990Z",
     "end_time": "2023-06-13T15:02:14.097000Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
